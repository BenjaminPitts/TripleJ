const alerts = JSON.stringify({"next_page": 2, "alerts": [{"status": "healthy", "metric": "alias(divideSeries(aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_processMicroblink.*.POST.status.5*.count:sum,'average'),aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_processMicroblink.*.POST.status.*.count:sum,'average')),'Percentage_5xx_Responses_for_processMicroblink')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.02}, "id": "ac0762cc-3832-450f-8076-761da79ad192", "currently_triggered_metrics": [], "info": "RCS is experiencing a higher than usual percentage of 5xx responses to /receipt/processMicroblink.  This is a new alert and may require tweaking. 2/1/2021", "on_query_failure": "notify", "name": "5xx Responses for /receipt/processMicroblink as percentage of total", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeries(aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_submit.*.POST.status.5*.count:sum,'sum'),aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_submit.*.POST.status.*.count:sum,'sum')),'Percentage_5xx_Responses_for_submit')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 15, "type": "above", "above_value": 0.05}, "id": "3dac8c0f-daf3-4c03-a1ba-f39f34076b00", "currently_triggered_metrics": [], "info": "RCS is experiencing a higher than usual percentage of 5xx responses for /receipt/submit.  This is a new alert and may require tweaking. 2/1/2021", "on_query_failure": "notify", "name": "5xx responses for /receipt/submit as percentage of total", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "movingAverage(aws.dataops-prod.ecs.us-east-1.cluster.prod-airflow-orchestration-cluster.service.*.MemoryUtilization,'1min')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 90.0}, "id": "dd66af30-558d-4a7b-b1dc-cd71b5c54d9c", "currently_triggered_metrics": [], "info": "Memory Utilization for the Airflow Orchestration service is above 90%.\n\nIdentify which airflow services/jobs Memory Util has spiked, monitor whether or not Mem Utilization is trending downwards, and adjust Fargate resources if the spike in utilization is sustained.", "on_query_failure": "notify", "name": "Airflow Cluster Memory Utilization alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "movingAverage(aws.dataops-prod.ecs.us-east-1.cluster.prod-airflow-orchestration-cluster.service.*.CPUUtilization,'5min')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 90.0}, "id": "7c0d86db-cfab-4c68-a554-fdbb3a1e2f2e", "currently_triggered_metrics": [], "info": "CPU Utilization for the Airflow Orchestration service is above 90%.\n\nIdentify which airflow services CPU has spiked, monitor whether or not CPU Utilization is trending downwards, and adjust Fargate resources if the spike in utilization is sustained.", "on_query_failure": "notify", "name": "Airflow ECS CPU Utilization alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "gauges.airflow.prod.executor.*", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 3.0}, "id": "094ef5a4-6d1b-4d9b-ac51-533303b4da3d", "currently_triggered_metrics": [], "info": "# of Available Executors  for the Airflow Orchestration service is less than 3.\n\nIdentify which airflow dags/tasks are hogging the executors, monitor whether or not the available # of executors is trending upwards, and increase # of workers if the spike in utilization is sustained.", "on_query_failure": null, "name": "Airflow Task Executor Metrics alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aws.dataops-prod.alb.us-east-1.inst.app.prod-airflow-webserver-alb.d44b27484d3a56ac.us-east-1d.targetgroup.prod-airflow-target-group.f8379947621e71dc.HealthyHostCount", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 0.5}, "id": "26143a9b-17ab-41c7-abf3-a9ecbcfea04c", "currently_triggered_metrics": [], "info": "Airflow Webserver is Down. Please take a look at cloud watch logs.", "on_query_failure": null, "name": "Airflow Webserver Health Check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aliasByNode(aws.appdev.lambda.us-east-1.function.prod-appsflyer-event-sender.Invocations,6)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 60, "type": "missing"}, "id": "a9cac13d-5467-49c5-8d57-964a6aa0d682", "currently_triggered_metrics": [], "info": "The appsflyer-event-sender Lambda is not being invoked as frequently as expected.", "on_query_failure": "notify", "name": "AppsFlyer S2S Event Sender Requests alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$.receiptDate.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.Android.attribute.receiptDate.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.85}, "id": "cf3cc3a7-9c68-4481-a4f7-cd8f8f93858b", "currently_triggered_metrics": [], "info": "Blink is missing receipt date at an abnormally high rate.", "on_query_failure": "notify", "name": "Blink Date Missing - Android", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$.receiptDate.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.iOS.attribute.receiptDate.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.85}, "id": "b24a9db4-5324-40e1-afb8-49f6a7d453c5", "currently_triggered_metrics": [], "info": "Blink is missing the receipt date at an abnormally high rate.", "on_query_failure": "notify", "name": "Blink Date Missing - iOS", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$.storeName.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.Android.attribute.storeName.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.95}, "id": "2f8336c8-4831-4c6e-ac81-fbe93fc7ac85", "currently_triggered_metrics": [], "info": "Blink is not reporting store name as often as expected. Check to make sure nothing has gone wrong.", "on_query_failure": "notify", "name": "Blink Store Name Missing - Android", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$.storeName.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.iOS.attribute.storeName.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.95}, "id": "9a28d14a-3514-4178-8819-c197e485e3a3", "currently_triggered_metrics": [], "info": "Blink is not reporting store name as often as expected. Check to make sure nothing has gone wrong.", "on_query_failure": "notify", "name": "Blink Store Name Missing - iOS", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$.receiptTime.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.Android.attribute.receiptTime.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.85}, "id": "773a1065-3249-488a-8a8f-ea91974626ae", "currently_triggered_metrics": [], "info": "Blink is not finding \"Time\" as often as expected. Check to see if they are having issues.", "on_query_failure": "notify", "name": "Blink Time Missing - Android", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$.receiptTime.string,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.iOS.attribute.receiptTime.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.85}, "id": "72361499-1940-4b49-9f5c-7792c8651f9a", "currently_triggered_metrics": [], "info": "Blink is not reporting the Time as often as expected. Check to make sure nothing is wrong on their end.", "on_query_failure": "notify", "name": "Blink Time Missing - iOS", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$.total.number,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.Android.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.Android.attribute.total.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.9}, "id": "327c8a6f-fb96-487e-ae6b-cec99c82f6a5", "currently_triggered_metrics": [], "info": "Blink is no reporting Total as often as expected for Android. Check to make sure things are OK on their end.", "on_query_failure": "notify", "name": "Blink Total Missing - Android", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeriesLists(summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$.total.number,4),'1d','sum',false),summarize(sumSeriesWithWildcards(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.iOS.$._id.$oid.string,4),'1h','sum',false)),'MicroblinkReceipts.iOS.atrribute.total.pct')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 120, "type": "below", "below_value": 0.9}, "id": "fefcbe68-19b9-4f87-b07a-9c4dee8a62dc", "currently_triggered_metrics": [], "info": "Blink is not reporting total as often as expected. Check and see if things are OK on their end.", "on_query_failure": "notify", "name": "Blink Total Missing - iOS", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["14f8ea1b-49ae-48c2-b8f6-6aa75d657ce8"], "additional_criteria": {}}, {"status": "healthy", "metric": "sum(aliasByNode(summarize(fetch.blink-accountability.prod-dataops.MicroblinkReceipts.*.*.$._id.$oid.string,'5min','sum',false),3,4,5,6,7,8,9))", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1}, "id": "849c567d-8c0a-4b2a-a61c-1b2661d7899c", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "blink-accountability-metrics", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "alias(aggregate(fetch.consumer-service.us-east-1.prod.endpoint.api_checklist_*.method.*.status.5*.count:sum,'count'),'500s')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 15, "type": "above", "above_value": 10.0}, "id": "fe3b0554-043c-40a5-93b8-cf860865b0b1", "currently_triggered_metrics": [], "info": "Checklist endpoints in Consumer Service are experiencing more 500s than anticipated", "on_query_failure": "notify", "name": "Checklist CS Endpoints alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.consumer-service-process-receipt.us-east-1.prod.endpoint.api_checklist_*.method.*.status.5*.count:sum,'500s')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 15, "type": "above", "above_value": 1.0}, "id": "787d717a-68ea-457e-b33e-8c80649de54d", "currently_triggered_metrics": [], "info": "Checklist endpoints in prod-consumer-service-process-receipt are responding with more 500s than anticipated.", "on_query_failure": "notify", "name": "Checklist CS Process Receipt Endpoints alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aliasByNode(aws.appdev.lambda.us-east-1.function.prod-checklist-receipt-worker.Errors,6)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 60, "type": "above", "above_value": 50.0}, "id": "8059c71d-6eb4-4aa3-852f-a425d70b46b2", "currently_triggered_metrics": [], "info": "prod-checklist-receipt-worker Lambda has experienced more errors or fewer invocations than expected.", "on_query_failure": "notify", "name": "Checklist Receipt Worker Requests alert", "muted": false, "scheduled_mutes": [], "expression": "a || b", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {"b": {"time_period": 60, "type": "missing", "metric": "aliasByNode(aws.appdev.lambda.us-east-1.function.prod-checklist-receipt-worker.Invocations, 6)"}}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_user_userId_reward.method.POST.status.404.count:sum,'404')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5.0}, "id": "2f325ec2-4627-4b99-91a2-5df564054d11", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Choose Preferred Reward alert", "muted": false, "scheduled_mutes": [], "expression": "a || b || c", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {"c": {"time_period": 5, "above_value": 5.0, "type": "above", "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_user_userId_reward.method.POST.status.5*.count:sum,'500')"}, "b": {"time_period": 5, "above_value": 5.0, "type": "above", "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_user_userId_reward.method.POST.status.400.count:sum,'400')"}}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_referral_referral-code_userId.method.POST.status.4*.count:sum,'400s')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.0}, "id": "ea02ec3b-4db9-437e-8454-0505606827fc", "currently_triggered_metrics": [], "info": "This alarm indicates that responses for the Consumer Service endpoint used to create and assign a user a referral code when they sign-up are failing unexpectedly.\n\napi/referral/referral-code/{userId}", "on_query_failure": "notify", "name": "Create Referral Code For User alert", "muted": false, "scheduled_mutes": [], "expression": "a || b || c", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {"c": {"time_period": 5, "type": "missing", "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_referral_referral-code_userId.method.POST.status.2*.count:sum,'200')"}, "b": {"time_period": 5, "above_value": 0.0, "type": "above", "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_referral_referral-code_userId.method.POST.status.5*.count:sum,'500')"}}}, {"status": "healthy", "tags": [], "metric": "aws.appdev.lambda.*.function.prod-cw-metrics-hg-forwarder.Duration", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 900000.0}, "id": "0deafa29-d2d5-460f-ad05-cca940bc7824", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "cw-metrics-hg-forwarder Duracion", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aliasByNode(aws.appdev.lambda.us-east-1.*.prod-cw-metrics-hg-forwarder.Errors,1,3,5,6)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 1.0}, "id": "21794f4a-1995-44cc-8cf3-d2d287f14685", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "cw-metrics-hg-forwarder Errors Alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.aws.quotas.us-east-1.cloudwatch.metricalarms.count", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 9000}, "id": "2e6eab9d-110d-4f57-86bd-c1e632acfed4", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "cwalarmscount", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "movingAverage(aws.dataops-prod.ecs.us-east-1.cluster.prod-dataops-dbt-serve-cluster.service.*.MemoryUtilization,'5min')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 90.0}, "id": "a75ecabe-06d4-4724-b1fe-0098d1d6493b", "currently_triggered_metrics": [], "info": "Memory Utilization for the DBT Docs Orchestration service is above 90%.\n\nIdentify which airflow services/jobs Memory Util has spiked, monitor whether or not Mem Utilization is trending downwards, and adjust Fargate resources if the spike in utilization is sustained.", "on_query_failure": "notify", "name": "DBT Docs Cluster Memory Utilization alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "movingAverage(aws.dataops-prod.ecs.us-east-1.cluster.prod-dataops-dbt-serve-cluster.service.*.CPUUtilization,'5min')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 90.0}, "id": "2880aeac-c9e0-4762-bbea-a3204a70787c", "currently_triggered_metrics": [], "info": "CPU Utilization for the DBT Docs Orchestration service is above 90%.\n\nIdentify which airflow services/jobs CPU has spiked, monitor whether or not CPU Utilization is trending downwards, and adjust Fargate resources if the spike in utilization is sustained.", "on_query_failure": "notify", "name": "DBT Docs ECS CPU Utilization alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.$env.ereceipt.amazon.processMicroblink.successProcessMicroblinkRCS.count:sum,5,7)", "notification_type": ["every", 15], "alert_criteria": {"time_period": 1, "type": "below", "below_value": 5}, "id": "a1c72451-eeaa-439e-bbde-ed500fdf5bdf", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Drop Dead Blink Amazon eReceipt Successful Submission Alarm", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.$env.ereceipt.email.processMicroblink.successProcessMicroblinkRCS.count:sum,5,7)", "notification_type": ["every", 15], "alert_criteria": {"time_period": 1, "type": "above", "above_value": 5}, "id": "f1e28119-8798-44be-9e87-140d3d9b6004", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Drop Dead Blink Email eReceipt Successful Submission Alarm", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.prod.ereceipt.processMicroblink.successProcessMicroblinkRCS.count:sum,6)", "notification_type": ["every", 15], "alert_criteria": {"time_period": 1, "type": "below", "below_value": 5}, "id": "d7b8728e-641b-419e-8701-d619690f870d", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Drop Dead Blink eReceipt Submission Alarm", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.ereceipt-submitter.prod.us-east-1._successfully_parsed:sum)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 20, "type": "below", "below_value": 5}, "id": "df5ad6cc-d251-4ac4-a5bd-0b0099ea9a30", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Drop Dead Ereceipt Submitter  Alarm", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.ereceipt-submitter.prod.us-east-1.err_amazon_parser_exception:sum)", "notification_type": ["every", 15], "alert_criteria": {"time_period": 1440, "type": "above", "above_value": 5}, "id": "7a49f1b9-bc8a-4e34-86a0-dc642039bec4", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "Ereceipt Submitter Amazon Parser  Failure alarm", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.ereceipt-metrics-lambda.prod.*.platform.*.FINISHED:sum", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 5}, "id": "f40119fe-9077-4576-91b4-5865c2451e38", "currently_triggered_metrics": [], "info": "This alarm is the \"drop dead\" alarm for ereceipts finishing and the level is a work in progress.", "on_query_failure": null, "name": "Ereceipts - All Finished", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6f4540a9-152f-43eb-8ae3-24092aef014d"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "sumSeriesWithWildcards(fetch.fastpipe.us-east-1.prod.fetch-debit-payment-service-cdc-connector.record-count.failed,5)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.0}, "id": "8c2e7e52-2fe3-4c4c-a75c-454e9cd72fb6", "currently_triggered_metrics": [], "info": "Fetch debit pipeline has failed to process records.\nPlease check out  CloudWatch logs of prod-<table_name>-dynamokds-fetch-debit  lambda functions", "on_query_failure": "notify", "name": "Failed record count alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["d2a9c409-d1bf-44f8-b84a-15086240f4e7", "34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "alerting", "tags": [], "metric": "sumSeriesWithWildcards(aws.appdev.lambda.*.function.prod-redis-monitoring-hg-forwarder.metric_count.failed.*,5)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.0}, "id": "4957dec4-20d4-4164-8163-f02db8490eca", "currently_triggered_metrics": ["aws.appdev.lambda.us-east-1.function.metric_count.failed.exception_FailedtotoconnecttoRedishostnameprodusercache20001004rfhzvv0001use1cacheamazonawscomnodeid0001Error111connectingtoprodusercache20001004rfhzvv0001use1cacheamazonawscom6379Connectionrefused", "aws.appdev.lambda.us-east-1.function.metric_count.failed.exception_FailedtotoconnecttoRedishostnameprodusercache20002004rfhzvv0001use1cacheamazonawscomnodeid0001Error111connectingtoprodusercache20002004rfhzvv0001use1cacheamazonawscom6379Connectionrefused"], "info": "prod-redis-monitoring-hg-forwarder lambda function failed to process Redis metrics\nPlease check out CloudWatch logs of  prod-redis-monitoring-hg-forwarder and Devops/Applications/redis-monitoring-hg-forwarder HG dashboard", "on_query_failure": "notify", "name": "Failures alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["d2a9c409-d1bf-44f8-b84a-15086240f4e7", "fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(transformNull(scale(divideSeries(sumSeries(sumSeries(fetch.fraud-fighter.us-east-1.prod.fraud-risk-assessment.BOUNCED_SIGNUP_EMAIL.DECLINED.count:sum),alias(sumSeries(fetch.fraud-fighter.us-east-1.prod.fraud-risk-assessment.BOUNCED_SIGNUP_EMAIL.*.count:sum),'total-requests')),100),0),'all-errors-percentage')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 20, "type": "above", "above_value": 10}, "id": "38482804-b0da-4102-ab10-95447cb64f69", "currently_triggered_metrics": [], "info": "This alert usually means there is an ATO attack going on. \n\nHere is the dashboard --> https://www.hostedgraphite.com/725fab9f/grafana/d/FEJR4PJGz/fraud-fighter-dashboard?orgId=2&fullscreen&edit&panelId=6&var-environment=prod&var-eventType=All&var-decision=All", "on_query_failure": "ignore", "name": "Fraud Fighter - High number of bounced email declines", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "metric": "movingSum(fetch.fraud-fighter.us-east-1.prod.fraud-risk-assessment.BOUNCED_SIGNUP_EMAIL.*.count:sum,'20min')", "notification_type": ["state_change"], "alert_criteria": {"time_period": 30, "type": "above", "above_value": 50}, "id": "0b96e1da-bd8d-4c01-a9bf-56ecda17ea57", "currently_triggered_metrics": [], "info": "The notification is for the number of signup emails hard bounced is high. This potentially means that there is a relatively high amount of fraudulent activity in registrations.\n\nTo handle this alarm follow up with the fraud team leads and monitor incoming user signups.\n\nDashboard - https://www.hostedgraphite.com/725fab9f/grafana/d/FEJR4PJGz/fraud-fighter-dashboard?orgId=2&var-environment=prod&var-eventType=BOUNCED_SIGNUP_EMAIL&var-decision=All", "on_query_failure": "ignore", "name": "Fraud Fighter - Kount Bounced signup email events HIGH", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["39fc5015-e6ae-4a93-95c7-6b16049ed16b"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(sumSeries(fetch.fraud-fighter.us-east-1.prod.fraud-risk-assessment.BOUNCED_SIGNUP_EMAIL.*.count:sum),7)", "notification_type": ["state_change"], "alert_criteria": {"time_period": 150, "type": "missing"}, "id": "0695aebb-b436-48b0-8e7b-0bdefeca1324", "currently_triggered_metrics": [], "info": "The notification is for the number of signup emails hard bounced are not generated for a long time. This potentially means that there is no fraudulent activity in registrations or iterable is not delivering hard-bounced email data to our webhook. \n\nPlease follow up with Jessica B or iterable\n\nDashboard - https://www.hostedgraphite.com/725fab9f/grafana/d/FEJR4PJGz/fraud-fighter-dashboard?editPanel=6&orgId=2&var-environment=prod&var-eventType=BOUNCED_SIGNUP_EMAIL&var-decision=All", "on_query_failure": "ignore", "name": "Fraud Fighter - Kount Bounced signup email events not generating", "muted": false, "scheduled_mutes": ["9d224701-f7cd-43a9-926e-40d200f1056d"], "expression": "a", "notification_channels": ["39fc5015-e6ae-4a93-95c7-6b16049ed16b"], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(sumSeries(fetch.fraud-fighter.us-east-1.prod.fraud-risk-assessment.REGISTRATION.*.count:sum),7)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "missing"}, "id": "bcb35528-0649-4bdd-bbc5-2a30099f596d", "currently_triggered_metrics": [], "info": "This alert means we are not seeing user registration events. Good starting points would be CS configuration for Fraud fighter, or the app is down. \n\nDashboard for the metric --> https://www.hostedgraphite.com/725fab9f/grafana/d/FEJR4PJGz/fraud-fighter-dashboard?orgId=2&fullscreen&edit&panelId=6", "on_query_failure": "ignore", "name": "Fraud Fighter - Kount Registration events not generating", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.receipt_service.us-east-1.prod.GCTime:95pct,\"Microseconds_Paused_For_GC\")", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5000.0}, "id": "11c6cc51-94dd-4df2-8876-e2a9c326a0dc", "currently_triggered_metrics": [], "info": "Receipt service is experiencing unusually high garbage collection pause times.  This is a new alert and may require tweaking.", "on_query_failure": "notify", "name": "Garbage Collection (microseconds paused) alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_referral_user_userId.method.GET.status.500.count:sum,'500')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5.0}, "id": "16bb8cfc-bdf0-4acc-9902-db415e851b6e", "currently_triggered_metrics": [], "info": "This alarm indicates unexpected 500s are occurring on the following endpoint:\n\nResponses for the endpoint used to retrieve a user's referral code and high-level referral stats\n\napi/referral/user/{userId}", "on_query_failure": "notify", "name": "Get User Referral Info alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {}}, {"status": "healthy", "metric": "_hg_meta.traffic.datapoints.*.received", "notification_type": ["state_change"], "alert_criteria": {"above_value": 130000, "type": "above"}, "id": "a61ffcf3-3893-42b0-9a24-b9b96088468c", "currently_triggered_metrics": [], "info": "Metrics Datapoints received approaching contractual limit with Hosted Graphite.\n\nPlease see: https://www.hostedgraphite.com/725fab9f/grafana/d/ekQeYXZGk/hg-traffic-dashboard?orgId=2&refresh=5m", "on_query_failure": null, "name": "HG Approaching Datapoint Limit", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "_hg_meta.traffic.concurrent_metrics.received", "notification_type": ["state_change"], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 700000}, "id": "deebf0c2-f836-4a33-802c-56b91a256728", "currently_triggered_metrics": [], "info": "This is indicating that we are reaching our Live Metrics limit with hosted graphite.", "on_query_failure": null, "name": "HG Live Metrics Alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "_hg_meta.metric_stats.total_metrics", "notification_type": ["state_change"], "alert_criteria": {"above_value": 600000, "type": "above"}, "id": "96aeb716-731a-49fd-a697-ff2bdeee98c0", "currently_triggered_metrics": [], "info": "This is generally an indicator of High Cardinality, if the \"Live Metrics\" are not reaching the same value as \"Total Metrics\"\n\nSee: https://www.hostedgraphite.com/725fab9f/grafana/d/ekQeYXZGk/hg-traffic-dashboard?orgId=2&refresh=5m", "on_query_failure": null, "name": "HG Total Metrics Warning - Above 600k", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "sumSeries(_hg_meta.traffic.datapoints.*.{invalid,dropped})", "notification_type": ["every", 2880], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 10}, "id": "1cb5735a-a41c-4bc4-9225-c0a82c09ab05", "currently_triggered_metrics": [], "info": "Alert if datapoint are being dropped due to ratelimiting or being malformed. This alert does not count towards the overall alert limit.", "on_query_failure": "notify", "name": "HG-Alert: Datapoints dropped.", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "metric": "asPercent(fallbackSeries(_hg_meta.traffic.concurrent_metrics.received,constantLine(1)),fallbackSeries(keepLastValue(_hg_meta.traffic.concurrent_metrics.limit),constantLine(1000)))", "notification_type": ["every", 2880], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 80}, "id": "14f2120f-ccf8-4ef0-85b8-d70db267bfd5", "currently_triggered_metrics": [], "info": "Alert if the live metrics are above 80% of live metric limit. Going above the live metric limit will cause metrics to be dropped. This alert does not count towards the overall alert limit.", "on_query_failure": "notify", "name": "HG-Alert: Live metrics above 80% of limit.", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.brand-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "2274fa7e-874e-41a9-89fc-4e14026d6a6b", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-brand-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.consumer-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "6c29f377-e8bd-4a6d-afe4-ab1a7d52f309", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-consumer-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.fetchrewards-com.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "d70bde01-9e59-40ee-b036-46d853110c04", "currently_triggered_metrics": [], "info": "fetchrewards-com is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-fetchrewards-com-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.holdouts-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "d499b4ff-0509-41c1-8c4d-4ba8b7116a77", "currently_triggered_metrics": [], "info": "holdouts-service is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-holdouts-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.identity-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "5df38235-269d-4301-b5f9-8306212371eb", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-identity-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.loyalty-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "a464a58d-dae6-4d98-8ffb-26bb5c45fd2a", "currently_triggered_metrics": [], "info": "loyalty-service is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-loyalty-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.offer-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "ca8a12ab-0c03-473b-ad34-04a10a2daf0d", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-offer-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.prod-cust-serv-fetchrewards-com.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "666493c1-5a97-49fe-b2d4-b916159b1e9a", "currently_triggered_metrics": [], "info": "prod-cust-serv-fetchrewards-com is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-prod-cust-serv-fetchrewards-com-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.product-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "bd5c3974-6f02-4be1-b6fa-1e893f736827", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-product-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.purchase-history-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "99396a37-fa9d-48d1-98a4-0d3223b43b8d", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-purchase-history-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.receipt-capture-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "1c024f2e-cdc1-4ed9-ae68-4fa4efb52163", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-receipt-capture-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.receipt-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "f013d5ef-1e3e-4e38-8a9c-c17144b1649c", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-receipt-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.rewards-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "f67e13fd-5c81-46b6-bef5-7e150d181353", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "hg-p3-rewards-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.websocket-event-service.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "0b96d74b-712b-49d2-9fc1-2cbbe160805b", "currently_triggered_metrics": [], "info": "websocket-event-service is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-websocket-event-service-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.apimonitor.*.*.www-fetchrewards-com.*.*.*.up", "notification_type": ["state_change"], "alert_criteria": {"time_period": 2, "type": "below", "below_value": 1.0}, "id": "46141d0e-458e-4d97-8c0c-99826ddd7f52", "currently_triggered_metrics": [], "info": "www-fetchrewards-com is not sending successful healthchecks to HG for last 2 minutes", "on_query_failure": null, "name": "hg-p3-www-fetchrewards-com-updown-check", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e26b4199-a3b3-4836-8a58-9f4ed4bb9b7f"], "additional_criteria": {}}, {"status": "healthy", "metric": "aws.prod-mlops.sagemaker.us-east-1.endpoint.*.variant.*.modellatency.p99_microseconds", "notification_type": ["every", 60], "alert_criteria": {"time_period": 15, "type": "above", "above_value": 100000}, "id": "620be399-683d-474f-bbf7-4d5abf5d59cd", "currently_triggered_metrics": [], "info": "Model latency for some endpoint has risen above 100ms for 15 minutes", "on_query_failure": null, "name": "High model latency", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["28426bc7-f748-43e6-869a-10ed4e652a00"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.consumer-service.us-east-1.prod.endpoint.api_kount_redemption-event.method.POST.status.400.count:sum,sumSeries(fetch.consumer-service.us-east-1.prod.endpoint.api_kount_redemption-event.method.POST.status.*.count:sum))", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 0.5}, "id": "d2c46dbf-573b-438a-96f7-5b36b8548f67", "currently_triggered_metrics": [], "info": "The number of redemptions being rejected for fraud reasons is high. (Note that despite the method name, these rejects are not just from Kount, they also include our own internal rules.)\n\nThis potentially means that there is a problematic rule defined in Kount that is overly aggressive in its rejections. Or it means that there is a high amount of fraudulent activity.\n\nTo handle this alarm follow up with the fraud team and have them double check any new fraud rules.", "on_query_failure": "notify", "name": "High Number Of Rewards Redemptions Rejected", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["799242a2-c10f-40c0-b0a3-70d3b35e9627", "17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.rewards-service*.prod.us-east-1.endpoint.rewards_validate-redemption.method.POST.status.200.count:sum,aggregate(fetch.rewards-service*.prod.us-east-1.endpoint.rewards_validate-redemption.method.POST.status.*.count:sum,'sum'))", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "below", "below_value": 0.65}, "id": "65ad1b93-9499-4420-8e11-e200f35d3b0e", "currently_triggered_metrics": [], "info": "A high percentage of rewards redemption validation attempts are failing. Known culpretes for this alarm: we broke sending 2FA codes somehow, or fraudsters are spamming our redemptions (usually only happens over night)\n\nhttps://fetchrewards.atlassian.net/wiki/spaces/AS/pages/2447016028/How+to+Handle+High+Percentage+Of+Redemption+Validations+F\n\nThe Redemption Metrics dashboard, https://www.hostedgraphite.com/725fab9f/grafana/d/cHiOvqoMz/redemption-metrics?refresh=10s, is a good place to start troubleshooting.", "on_query_failure": "ignore", "name": "High Percentage Of Redemption Validations Failing", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.preprod.us-east-1.holdouts.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "a3097946-a5b4-45b1-b821-763121e3d02f", "currently_triggered_metrics": [], "info": "preprod holdouts pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- Lambda function errors in preprod-holdouts-worker\n- malformed incoming offers, brands, CPGs from SNS preprod-saved-offers, preprod-saved-brands, ...\nThe first thing to do is to check the Lambda function's CW logs", "on_query_failure": null, "name": "Holdouts Pipeline Unhealthy (preprod)", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.prod.us-east-1.holdouts.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "f80e30f4-338d-418a-ac69-72fb85e137e6", "currently_triggered_metrics": [], "info": "prod holdouts pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- Lambda function errors in prod-holdouts-worker\n- malformed incoming offers, brands, CPGs from SNS prod-saved-offers, prod-saved-brands, ...\nThe first thing to do is to check the Lambda function's CW logs", "on_query_failure": null, "name": "Holdouts Pipeline Unhealthy (prod)", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.image-service.prod.us-east-1.endpoint.*.method.GET.status.500.count", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 15.0}, "id": "1c25e7e2-225a-4b61-af79-d94a4f20e67b", "currently_triggered_metrics": [], "info": "prod-image-service is experiencing 500s at an alarming rate", "on_query_failure": null, "name": "Image Service 500 alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "sumSeriesWithWildcards(aws.appdev.kinesis.us-east-1.stream_name.prod-fetch-debit-*-kds.PutRecords.FailedRecords,5)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.0}, "id": "3d799a4c-9995-46ab-b082-9753de92a93b", "currently_triggered_metrics": [], "info": "Fetch debit pipeline has failed to process records.\nPlease check out  CloudWatch logs of prod-<table_name>-dynamokds-fetch-debit  lambda functions", "on_query_failure": "notify", "name": "KDS fetch-debit put_record failure alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-eligibility-snapshots-processor.prod.us-east-1.kinesis_overloaded", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 100.0}, "id": "43e50094-800d-4369-92ee-625b76b02348", "currently_triggered_metrics": [], "info": "Kinesis is overloaded in the offer snapshots pipeline, records are getting throttled on 100 attempts in 5 mins.\nKinesis is the bottleneck - likely fix is to increase Kinesis shards via Kinesis console. Also check on Redis and offer-service volume/errors.\n\nParts:\nRedis: offers-eligibility-snapshots-db\nLambda: offer-eligibility-snapshots-processor\nKinesis: offer-eligibility-snapshots-kds\n\nwiki: https://fetchrewards.atlassian.net/wiki/spaces/OP/pages/2037809603/Offer+Eligibility+Snapshots+Pipeline", "on_query_failure": null, "name": "Kinesis Overloaded (Not all records sent successfully) alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(transformNull(scale(divideSeries(sumSeries(fetch.consumer-service.us-east-1.prod.fraud.Kount.errors.*.*.count:sum),alias(sumSeries(fetch.consumer-service.us-east-1.prod.fraud.Kount.requests.*.count:sum),'total-requests')),100),0),'all-errors-percentage')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5}, "id": "eac8643e-8aa6-4e9a-b395-73dbb6537f65", "currently_triggered_metrics": [], "info": "Alerting that 5% of Kount calls have resulted in an error.", "on_query_failure": "notify", "name": "Kount Error Alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(sumSeries(fetch.consumer-service.us-east-1.prod.user.creation.reject.KountRegistrationRejectionException.appVersion.*.platform.*.count:sum),sumSeries(fetch.consumer-service.us-east-1.prod.user.creation.*.appVersion.*.platform.*.count:sum,fetch.consumer-service.us-east-1.prod.user.creation.reject.KountRegistrationRejectionException.appVersion.*.platform.*.count:sum))", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 0.5}, "id": "c41344ad-8d2e-45c9-bd17-32a8b62df6e7", "currently_triggered_metrics": [], "info": "An excessive percentage of user sign ups are being rejected by Kount. This potentially means that there is a problematic rule defined in Kount that is overly aggressive in its rejections. Or it means that there is a high amount of fraudulent activity (as a proportion of overall traffic).\n\nTo handle this alarm follow up with the fraud team and have them double check any new fraud rules for registrations. If this alarm goes off outside of peak hours, it is likely that there is just a lot of fraud.", "on_query_failure": "notify", "name": "Kount User Creation Rejection", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["799242a2-c10f-40c0-b0a3-70d3b35e9627", "17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(sumSeries(fetch.consumer-service.us-east-1.prod.user.demographicUpdate.reject.KountRegistrationRejectionException.appVersion.*.platform.*.count:sum),sumSeries(fetch.consumer-service.us-east-1.prod.user.demographicUpdate.reject.KountRegistrationRejectionException.appVersion.*.platform.*.count:sum,fetch.consumer-service.us-east-1.prod.user.demographicUpdate.*.appVersion.*.platform.*.count:sum))", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 0.5}, "id": "64e4dbe6-dcdb-4d3f-bdba-a70933741134", "currently_triggered_metrics": [], "info": "The percentage of user updates rejected by Kount is unreasonably high. It's likely the fraud team has mistakenly made the fraud rules too aggressive.", "on_query_failure": "notify", "name": "Kount User Update Rejections", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["799242a2-c10f-40c0-b0a3-70d3b35e9627", "17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "sumSeriesWithWildcards(aws.appdev.lambda.*.function.prod-redis-monitoring-hg-forwarder.Errors,5)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 0.0}, "id": "ebd530ba-ead4-4582-8052-c379b1b714cc", "currently_triggered_metrics": [], "info": "prod-redis-monitoring-hg-forwarder lambda functions failed to send Redis metrics to HG.\nPlease check out CloudWatch logs of prod-redis-monitoring-hg-forwarder lambda function", "on_query_failure": "notify", "name": "Lambda failures (from CloudWatch) alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["d2a9c409-d1bf-44f8-b84a-15086240f4e7", "fa8f9890-f5a6-4d24-adaa-7e831b6a4138"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.rewards-service.prod.us-east-1.RewardRedemption.submitThirdPartyRedemption.count:sum,fetch.rewards-service.prod.us-east-1.endpoint.rewards_start-redemption.method.POST.status.200.count:sum)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "below", "below_value": 0.25}, "id": "0798a6fb-9482-45cb-a9d7-dcba2d69c9d2", "currently_triggered_metrics": [], "info": "The ratio of \"start redemption\" calls to \"award gift card\" calls is low. This means many attempts to redeem rewards are not resulting in completed redemptions.\n\nIOU wiki page, in the meantime, start with the Redemption Metrics dashboard, https://www.hostedgraphite.com/725fab9f/grafana/d/cHiOvqoMz/redemption-metrics?orgId=2&refresh=10s", "on_query_failure": "ignore", "name": "Low Percentage Of Started Redemptions Completed", "muted": false, "scheduled_mutes": ["02c420fc-6ccd-46a8-920e-a1eebbbfa9d6"], "expression": "a", "notification_channels": ["17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "hg_agent.tableau.prod.us-east-1.*.cpu.cpu0.system", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 90.0}, "id": "87fa9e3d-e811-4eff-b37b-2f45efc9a30c", "currently_triggered_metrics": [], "info": "CPU Util > 90 for Prod EC2 Tableau Internal Server. Please log in to the server i-0157213a6fbf4e479 and to find out the root cause of high CPU util.", "on_query_failure": null, "name": "Max of system.cpu.system over host:prod-tableau by host alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["34de1c54-2bf9-4d0a-9123-82515ea107bb"], "additional_criteria": {}}, {"status": "healthy", "metric": "aws.prod-mlops.sagemaker.us-east-1.endpoint.*.variant.*.cpuutilization.average_percent", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 95}, "id": "b7891717-2355-452d-ad84-901e4e14f70a", "currently_triggered_metrics": [], "info": "CPU Utilization has been above 95% for some model endpoint for 10 minutes", "on_query_failure": null, "name": "Model Endpoint CPU Util", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["28426bc7-f748-43e6-869a-10ed4e652a00"], "additional_criteria": {}}, {"status": "healthy", "metric": "aws.prod-mlops.sagemaker.us-east-1.endpoint.*.variant.*.diskutilization.average_percent", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 95}, "id": "be6cd3a6-e136-4aae-9b43-c3fd8c6ed931", "currently_triggered_metrics": [], "info": "Disk utilization for some model endpoint has been above 95% for 10 minutes", "on_query_failure": null, "name": "Model Endpoint Disk Util", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["28426bc7-f748-43e6-869a-10ed4e652a00"], "additional_criteria": {}}, {"status": "healthy", "metric": "aws.prod-mlops.sagemaker.us-east-1.endpoint.*.variant.*.memoryutilization.average_percent", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 95}, "id": "a5112ac3-c732-4bb4-b70a-b0deac0d7afe", "currently_triggered_metrics": [], "info": "Memory utilization for some model endpoint has been above 95% for 10 minutes", "on_query_failure": null, "name": "Model Endppint Mem Util", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["28426bc7-f748-43e6-869a-10ed4e652a00"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.preprod.us-east-1.offer_data_sync.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "71cdb57a-c440-4873-9a30-1d85db552f43", "currently_triggered_metrics": [], "info": "preprod offer-data-sync pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- Lambda function errors in preprod-offer-data-sync-worker\n- malformed incoming offers, offer usages, and users from SNS preprod-users-sync, -brands-sync, -offers-sync\nThe first thing to do is to check the Lambda function's CW logs", "on_query_failure": null, "name": "Offer Data Sync Pipeline Unhealthy (preprod)", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.prod.us-east-1.offer_data_sync.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "e97ff6b3-f108-49e5-92c2-6d6e79d41ce0", "currently_triggered_metrics": [], "info": "prod offer-data-sync pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- Lambda function errors in prod-offer-data-sync-worker\n- malformed incoming offers, offer usages, and users from SNS prod-users-sync, -brands-sync, -offers-sync\nThe first thing to do is to check the Lambda function's CW logs", "on_query_failure": null, "name": "Offer Data Sync Pipeline Unhealthy (prod)", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "metric": "sum(fetch.offer-service.us-east-1.prod.httpServerRequests.exception.none.method.*.status.500.uri.*.m5_rate)", "notification_type": ["state_change"], "alert_criteria": {"above_value": 100, "type": "above"}, "id": "5d2e7119-25ec-4e7a-8d37-21880f9f3763", "currently_triggered_metrics": [], "info": "This alert triggers when we begin to experience a high number of 500 errors in a 5 minute period.\n\nTalk to: Jacob Biggs, #pack-offers", "on_query_failure": "notify", "name": "Offer Service 5 Minute 500 Count", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["e42942e7-7e8a-4033-afeb-0edd50a1f5b3"], "additional_criteria": {}}, {"status": "healthy", "metric": "fetch.rewards-service.prod.us-east-1.requests.startexecution.prod-delay-reward-processor2.count:sum", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 1}, "id": "ae772309-a3a7-4c4c-b505-ee8a75bfbef4", "currently_triggered_metrics": [], "info": "This means we have reached the capacity of our delayed redemption step function. We should contact AWS to raise the concurrent execution limit.", "on_query_failure": null, "name": "Overflow Delayed Redemption Step Function Utilized", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["17292567-6316-4cf7-99e6-dea706d8d60e"], "additional_criteria": {}}, {"status": "healthy", "metric": "appdev-dev-us-east-1-pam-service-awseb-AWSEB-51U45121ZP5K", "notification_type": ["state_change"], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 3}, "id": "8dbc675a-908a-4ae4-a32a-44510581feea", "currently_triggered_metrics": [], "info": null, "on_query_failure": null, "name": "PAM Service - High LB Response Time", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["0607f266-226c-43b6-8eba-1c49ed88ac4b"], "additional_criteria": {}}, {"status": "healthy", "metric": "alias(divideSeries(aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_submit.*.POST.status.400.count:sum,'average'),aggregate(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_submit.*.POST.status.*.count:sum,'average')),'Percentage400Responses')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 10, "type": "above", "above_value": 0.4}, "id": "61af55ca-8dad-45b4-afc9-fa72e95c3dbe", "currently_triggered_metrics": [], "info": "This is a new alert and may require tweaking. 2/1/2021", "on_query_failure": "notify", "name": "Percentage of 4xx responses for /receipt/submit is high", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "alias(fetch.consumer-service.us-east-1.prod.endpoint.api_user_points-transaction_userId.method.PUT.status.5*.count:sum,'500CSmobile')", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5.0}, "id": "25792486-7e68-4614-9353-ddebf7041877", "currently_triggered_metrics": [], "info": "This alarm indicates an issue with the Points Transactions endpoint (api/user/points-transactions/{userId}) in Consumer Service. 500s are being experienced, which means some users are not being awarded points when they should.", "on_query_failure": "notify", "name": "Points Transactions alert", "muted": false, "scheduled_mutes": [], "expression": "a || b || c", "notification_channels": ["c99f4d18-6bb5-4806-afc0-c70f7baa64cd", "45ee800b-bc06-4ceb-8c49-6c91617429f4"], "additional_criteria": {"c": {"time_period": 5, "above_value": 5.0, "type": "above", "metric": "alias(fetch.consumer-service-dash.us-east-1.prod.endpoint.api_user_points-transaction_userId.method.PUT.status.5*.count:sum, '500 CS dash')"}, "b": {"time_period": 5, "above_value": 5.0, "type": "above", "metric": "alias(fetch.consumer-service-process-receipt.us-east-1.prod.endpoint.api_user_points-transaction_userId.method.PUT.status.5*.count:sum, '500 CS process-receipt')"}}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.preprod.us-east-1.purchase_history.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "736127b5-b257-409c-b647-4e50cc4ddf7c", "currently_triggered_metrics": [], "info": "preprod purchase history pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- Lambda function errors in preprod-purchase-history-worker\n- incoming receipts from prod-finished-receipts are not the expected format\nThe first thing to do is to check the Lambda function's CW logs and the purchase history worker HG dashboard.", "on_query_failure": null, "name": "preprod purchase history pipeline unhealthy", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.prod.endpoint.receipt_processMicroblink.method.POST.status.200.time.nano:95pct,5,9)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 450000000.0}, "id": "448ce095-18c7-430f-ab8d-6ecdd71481f7", "currently_triggered_metrics": [], "info": "RCS is experiencing high latency times. Note - the alarm may need to be tweaked.", "on_query_failure": "notify", "name": "Process Microblink p95 alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "fetch.offer-pipeline-tracer.prod.us-east-1.purchase_history.healthy", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "below", "below_value": 1.0}, "id": "8701c77b-84b4-4164-9f96-84da10137898", "currently_triggered_metrics": [], "info": "prod purchase history pipeline is unhealthy! Either the pipeline is down or it's very slow. This alert is caused by an offer-pipeline-tracer-worker pipeline test failure.\nTalk to: Russell Romney, #pack-offers\nLikely problems:\n- the Lambda function is malfunctioning\n- incoming receipts from prod-finished-receipts are not the expected format\nThe first thing to do is to check the Lambda function's CW logs and the purchase history worker HG dashboard.", "on_query_failure": null, "name": "prod purchase history pipeline unhealthy", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": [], "additional_criteria": {}}, {"status": "healthy", "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.prod.endpoint.*.*.method.*.status.500.count:sum,5,9)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 15, "type": "above", "above_value": 10}, "id": "89bcea89-9d30-407c-bea5-da4ecd7ce702", "currently_triggered_metrics": [], "info": "RCS is returning a high number of 500s - look and see what is going on.", "on_query_failure": "notify", "name": "RCS - Returning 500s", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["6edc53c5-e678-4376-81d7-1694b1521ebe"], "additional_criteria": {}}, {"status": "healthy", "tags": [], "metric": "aliasByNode(fetch.receipt-capture-service.us-east-1.prod.sns.newMicroblinkReceipt.publishFailure.count:sum,6)", "notification_type": ["every", 60], "alert_criteria": {"time_period": 5, "type": "above", "above_value": 5.0}, "id": "cf8f9183-4795-43c8-8768-1f83dc29eae6", "currently_triggered_metrics": [], "info": null, "on_query_failure": "notify", "name": "RCS Failed Microblink Receipt SNS Event Publish alert", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["d2a9c409-d1bf-44f8-b84a-15086240f4e7"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-activity-feed-db-001.0001.used_memory,fetch.prod.us-east-1.redis.prod-activity-feed-db-001.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "cee2254e-0b5d-4d12-ab87-69067140903a", "currently_triggered_metrics": [], "info": "prod-activity-feed-db-001's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-activity-feed-db-001's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-activity-feed-db-002.0001.used_memory,fetch.prod.us-east-1.redis.prod-activity-feed-db-002.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "00c38863-b66a-4563-aa09-66d19eabc39b", "currently_triggered_metrics": [], "info": "prod-activity-feed-db-002's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-activity-feed-db-002's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-app-version-cache-0001-001.0001.used_memory,fetch.prod.us-east-1.redis.prod-app-version-cache-0001-001.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "71f4b286-938c-45db-99dd-1beda1519d59", "currently_triggered_metrics": [], "info": "prod-app-version-cache-0001-001's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-app-version-cache-0001-001's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-app-version-cache-0001-002.0001.used_memory,fetch.prod.us-east-1.redis.prod-app-version-cache-0001-002.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "7bf29d97-28cf-4266-861f-2610f5ba9a8b", "currently_triggered_metrics": [], "info": "prod-app-version-cache-0001-002's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-app-version-cache-0001-002's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-app-version-cache-0001-003.0001.used_memory,fetch.prod.us-east-1.redis.prod-app-version-cache-0001-003.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "c6ac88fa-f643-4a41-996b-b135a6ace6b8", "currently_triggered_metrics": [], "info": "prod-app-version-cache-0001-003's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-app-version-cache-0001-003's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-auth-service-tokens-db-001.0001.used_memory,fetch.prod.us-east-1.redis.prod-auth-service-tokens-db-001.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "288b0fd1-eb5a-4437-9fc4-439c93bfba53", "currently_triggered_metrics": [], "info": "prod-auth-service-tokens-db-001's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-auth-service-tokens-db-001's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-auth-service-tokens-db-002.0001.used_memory,fetch.prod.us-east-1.redis.prod-auth-service-tokens-db-002.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "c48f4795-bb6e-49cc-8031-5eabf40ef3b3", "currently_triggered_metrics": [], "info": "prod-auth-service-tokens-db-002's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-auth-service-tokens-db-002's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-auth-service-users-db-001.0001.used_memory,fetch.prod.us-east-1.redis.prod-auth-service-users-db-001.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "527b537f-817d-4d61-b8b5-0735d99c8651", "currently_triggered_metrics": [], "info": "prod-auth-service-users-db-001's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-auth-service-users-db-001's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-auth-service-users-db-002.0001.used_memory,fetch.prod.us-east-1.redis.prod-auth-service-users-db-002.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "750935b7-546a-4693-994f-dd17d09de802", "currently_triggered_metrics": [], "info": "prod-auth-service-users-db-002's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-auth-service-users-db-002's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-brand-service-updates-db-001.0001.used_memory,fetch.prod.us-east-1.redis.prod-brand-service-updates-db-001.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "8338b155-5f77-45fd-89da-8d1149075c22", "currently_triggered_metrics": [], "info": "prod-brand-service-updates-db-001's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-brand-service-updates-db-001's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}, {"status": "healthy", "metric": "divideSeries(fetch.prod.us-east-1.redis.prod-brand-service-updates-db-002.0001.used_memory,fetch.prod.us-east-1.redis.prod-brand-service-updates-db-002.0001.maxmemory)", "notification_type": ["every", 60], "alert_criteria": {"above_value": 0.8, "type": "above"}, "id": "ed55055b-ad31-49f8-b705-4ea738ac8aeb", "currently_triggered_metrics": [], "info": "prod-brand-service-updates-db-002's memory utilization has exceeded 80% of the node's Total Memory.", "on_query_failure": "notify", "name": "Redis - prod-brand-service-updates-db-002's Total Memory Usage", "muted": false, "scheduled_mutes": [], "expression": "a", "notification_channels": ["b5e32be9-5044-452e-9a6e-392438e1d177"], "additional_criteria": {}}]}, null, 4);

console.log(alerts) 
